---
title: "R Notebook"
output: html_notebook
---

```{r}
rm(list = ls())
library(keras)
library(tidyverse)
library(imager)
library(readr)
library(tidyr)
library(tictoc)
library(rbenchmark)
```

# Download the Data
```{bash}
echo kaggle competitions download -c recognizing-faces-in-the-wild
```

```{r, echo = FALSE}
unzip('recognizing-faces-in-the-wild.zip',  overwrite = TRUE)
```

```{r}
if(!dir.exists('train'))
  unzip('train.zip', exdir = 'train')
if(!dir.exists('test'))
  unzip('test.zip', exdir = 'test')
```

# Create the Model
```{r}
parent_input <- layer_input(shape = c(224, 224, 3),
                            name = 'parent_input')

child_input <- layer_input(shape = c(224, 224, 3),
                           name = 'child_input')

parent <-parent_input %>%
  layer_conv_2d(filters = 32,
                kernel_size = c(3, 3),
                activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64,
                kernel_size = c(3, 3),
                activation = "relu",) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128,
                kernel_size = c(3, 3),
                activation = "relu",) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128,
                kernel_size = c(3, 3),
                activation = "relu",) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten()

child <- child_input %>%
  layer_conv_2d(filters = 32,
                kernel_size = c(3, 3),
                activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64,
                kernel_size = c(3, 3),
                activation = "relu",) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128,
                kernel_size = c(3, 3),
                activation = "relu",) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128,
                kernel_size = c(3, 3),
                activation = "relu",) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten()

predictions <- layer_concatenate(c(parent, child)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model <- keras_model(inputs = c(parent_input, child_input),
                     outputs = predictions)


model %>% compile(
  optimizer = optimizer_rmsprop(lr = 1e-4),
  loss = 'binary_crossentropy',
  metrics = c('accuracy')
)
```


# Read in CSV of kinship linking

```{r}
relationships <- read_csv('train_relationships.csv') %>%
  mutate(label = 1)

non_kin_relationship = tibble(
  p1 = sample(relationships$p1),
  p2 = sample(relationships$p2),
  label = 0
)

relationships = bind_rows(
  relationships,
  non_kin_relationship
)
```

```{r}
relationships <-  relationships %>%
  mutate(row_num = row_number()) %>%
  group_by(row_num) %>%
  mutate(
    p1 = file.path(getwd(), 'train', p1),
    p2 = file.path(getwd(), 'train', p2),
    n_files_p1 = length(list.files(p1)),
    n_files_p2 = length(list.files(p2))
  ) %>%
  filter(dir.exists(p1),
         dir.exists(p2),
         n_files_p1 >= 1,
         n_files_p2 >= 1) %>%
  ungroup
```

```{r}
relationships <- 
  relationships %>% 
  pivot_longer(
   cols = starts_with("p"),
   names_to = "p_column",
   values_to = "directory",
   values_drop_na = TRUE
  ) 
```

```{r}
relationships <- relationships %>% 
  transmute(
    p_column, directory, family_distinction = label, allowable_pairs = row_num
  )
```

```{r}
relationships_structured <- 
  relationships %>% 
  mutate(row_id = row_number()) %>% 
  split(., .$row_id) %>% 
  map_df(
    function(x) {
      tibble(directory = x$directory,
             filenames = file.path(directory, list.files(directory))) %>% 
        transmute(
          row_id     = x$row_id,
          filenames,
          family_distinction = x$family_distinction,
          allowable_pairs = x$allowable_pairs,
          p_column = x$p_column
        )
    }
  )
```

```{r}
all_relationships <- 
  relationships_structured %>% 
  split(
    .$allowable_pairs
  ) %>%
  map_df(
    function(x) {
      parent_child_list <- split(x, x$p_column)
      parent <-  parent_child_list$p1
      child <-  parent_child_list$p2
      map_df(
        parent$filenames,
        function(x) {
          child$parent_file = x
          child %>% 
            transmute(
              parent_file, 
              child_file = filenames,
              family_distinction,
              allowable_pairs,
              is_kin = TRUE)
        }
      )
    }
  ) 

all_relationships <- 
  all_relationships %>% 
  mutate(row_number = row_number())

```

create bad pairs
```{r}
shuffle_relationships <- 
  map_df(1:2,
      function(x) {
        all_relationships %>% 
        mutate(
          parent_num = sample(seq_along(parent_file)),
          child_num  = sample(parent_num),
          is_kin     = FALSE
        ) 
      })

order_parent_shuffle <- 
  shuffle_relationships %>% 
  transmute(parent_num, 
            parent_file, 
            family_distinction_parent = family_distinction) %>% 
  arrange(parent_num) %>% 
  select(-parent_num)

order_child_shuffle <- 
  shuffle_relationships %>% 
  select(child_num, 
         child_file, 
         family_distinction_child = family_distinction, 
         allowable_pairs, 
         is_kin) %>% 
  arrange(child_num) %>% 
  select(-child_num)


training_data <- bind_cols(order_parent_shuffle, order_child_shuffle) %>% 
  filter(family_distinction_parent != family_distinction_child) %>% 
  bind_rows(all_relationships) %>% 
  select(
    -family_distinction_child,
    -family_distinction_parent,
    -family_distinction,
    -row_number
  )
```

```{r}

  map2(
    all_relationships$parent_file[900],
    all_relationships$child_file[900],
    function(x, y) {
      x_image = load.image(x)
      y_image = load.image(y)
      plot(y_image)
      plot(x_image)
      
    }
  )
```


```{r}
datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE
)
```

```{r}
load_file <- 
  function(x) {
    x %>% 
    load.image() %>% 
    as.array() %>% 
    array_reshape(
      c(1, 224, 224, 3)
    )
  }

generate_pairs <- function(all_relationships, step_size = 10) {

  iteration = 1
  function(all_relationships, step_size) {
    max_steps = nrow(all_relationships)

    n_samples <- min(iteration+step_size - 1, max_steps)
    next_step <- all_relationships[iteration:n_samples,]
    if(n_samples == max_steps) 
      iteration <<- 1
    else 
      iteration <<- iteration+step_size 
    
    
    pf = map(
         next_step$parent_file,
         load_file
    )
    
    cf = map(
         next_step$child_file,
         load_file
    )
  
    list(
      array_reshape(pf, c(nrow(next_step), 224, 224, 3)), 
      array_reshape(cf, c(nrow(next_step), 224, 224, 3)),
      next_step$is_kin
    )
  }
}


pair_generator = generate_pairs()
pair_generator(training_data_shuffled[1:12,], step_size = 10) 
```

```{r}
# reps <- map_df(
#   1:10*10,
#   function(x) {
#    benchmark(
#      pair_generator(
#        all_relationships,
#        step_size = x
#      ),
#      replications = 1
#    ) 
#   }
# )
```

```{r}
shuffled_index = sample(1:nrow(training_data))
training_data_shuffled = training_data[shuffled_index,]
for(i in 1:100) {
  tic()
  gen_pairs <- pair_generator(training_data_shuffled, step_size = 10) 
  toc()
  model %>% fit(list(gen_pairs[[1]], 
                     gen_pairs[[2]]), 
                gen_pairs[[3]],
                epochs = 1)
  if (i%%5 == 0) {
    print('saving model')
    if(!dir.exists('models'))
      dir.create('models')
    save_model_hdf5(model, paste0('models/model_', i, '.hdf5'))
  }
}

```